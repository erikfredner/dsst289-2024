---
title: "07. Creating Data"
subtitle: "DSST289: Introduction to Data Science"
author: "Erik Fredner"
date: today
format:
  html:
    embed-resources: true
    link-external-newwindow: true
    df-print: paged
    smooth-scroll: true
    toc: true
editor:
  markdown:
    wrap: 72
---

```{r, include=FALSE, message=FALSE}
source("../funs/funs.R")

food <- read_csv(file.path("..", "data", "food.csv"))
food_prices <- read_csv(file.path("..", "data", "food_prices.csv"))
```

## Overview

It is a well-known aphorism within data science
that the majority of an analysis consists in cleaning and validating our data.
If we can collect our data in a clean format from the start, it will allow us
to proceed directly to the exploration stage once the data have been collected.

There are a number of excellent articles that give an extensive overview of how
to collect and organize data. Hadley Wickham's
["Tidy Data"](https://www.jstatsoft.org/article/view/v059i10),
one of the most cited papers across all of data science, offers an extensive
theoretical framework for describing a process for collecting data sets. (Wickham is also a core R developer, particularly of the `tidyverse`.)
Karl Broman and Kara Woo's
["Data Organization in Spreadsheets"](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989) offers a balance between practical advice and an extended discussion of general
principles for collecting data sets. The Data Carpentry guide
["Data Organization in Spreadsheets for Social Scientists"](https://datacarpentry.org/spreadsheets-socialsci/)
provides a concise list of common pitfalls.

These notes provide advice for organizing and storing data
within a spreadsheet program. Rather than an extensive discussion of various
pros and cons, it primarily focuses on the explicit approaches that I recommend.
For readers interested in a broader coverage, I suggest reading the articles
cited above. Because we are not using any fancy spreadsheet functions here, any
program that you would like to use should be fine. The screenshots come from
Microsoft Excel, but the same approach will work in Google Sheets, LibreOffice,
or another spreadsheet program of your choosing.

## Rectangular data

We have frequently discussed the concept of a rectangular data set, with
observations in rows and features in columns ("tidy data"). This is the same format that
we will use to collect our data. The first thing you will need to do, then, is
determine what *things* you are observing and what *properties* you would like
to collect about each thing. If you are observing different kinds of
things, each of which has a different set of associated properties, you may
need to store each set in a different table.

To match the format of rectangular data that we have been working with in R,
we need to structure our data set with a single row of column names, followed
by a row of data for each observation. Here is an example from Excel of a
nonsense data set with three features:

![Well-structured nonsense data](../imgs/excel1.png)

Notice that we need to always start in the first cell, A1, and fill in a
consistent number of rows and columns. We do not have multiple tables scattered
around the spreadsheet. We do not have multiple header columns. It is just the
data itself, stored in a rectangle in the upper-left-hand corner of
our spreadsheet, working right and down from cell A1. It is okay to include a small amount of formatting of the cells
to help with the data-entry process (I like to make the first row bold), but
do not try to record measurable properties of your data with formatting.

### Example of bad practice: color for meaning

::: {.callout-warning title="Do not record measurable properties with formatting"}
Do **not** use row highlights or color to indicate meaning in a spreadsheet. If you need to indicate a property of a row, create a new column and record the information there.
:::

Using highlight colors to indicate meaning is probably the most infamous data collection error. Here is an example of what *not* to do:

![Example of what *not* to do](../imgs/color-sheet.png)

In this **bad** example, colors indicate the type of food (fruit, vegetable, etc.) This is bad data for several reasons. First, it is impossible for a third party to know what each color means. Second, R cannot read the colors. Third, as a result, it is impossible to sort, filter, or otherwise process the data in R. (Fourth, it's ugly.)

If you would like to use color, create a new column and record the information there. Then, color the cells containing the property you made with conditional formatting.

Here are the conditional formatting instructions [in Excel](https://support.microsoft.com/en-us/office/use-conditional-formatting-to-highlight-information-in-excel-fed60dfa-1d3f-4e13-9ecb-f1951ff89d7f) and in [Google Sheets](https://support.google.com/docs/answer/78413?hl=en&co=GENIE.Platform%3DDesktop).

For example, using the original data set, you could set conditional formatting 
rules based on the `food_group` column:

![Setting up conditional formatting in the `food_group` column](../imgs/conditional-1.png)

When applied, those rules would automatically color additional cells matching
the condition(s) you set.

![Default conditional formatting values in Excel](../imgs/conditional-2.png)

In summary, you may use color to *understand* the data. But color **is not** data.

## Naming things

It is important to choose good feature names. As we have seen, the feature
names in a data set are used to describe
the graphics in the grammar of graphics and for manipulating data with verbs.
If our names are too complex or
difficult to remember, it will be more difficult to create data visualizations.
When feature names contain spaces or other special characters, it can become nearly
impossible to work within R without first cleaning up the feature names after
loading the data.

I find the best approach to feature names is to only use lower-case letters,
numbers, and underscores. The underscores can be used in place of spaces, but
do not make feature names more complex than needed. Also, make sure to start
the name of a feature with a lower-case letter (starting with a number is
invalid in R and many other programming languages). Note that I recommend using
lowercase even for feature names that should be capitalized, such as acronyms
and proper nouns. If you selectively capitalize, you will always need to
remember where this was done. In my scheme, it is one less thing to remember.

Throughout the rest of these notes, continuing with the food theme, we will
show examples of a small data set collecting information about a set of
cookbooks. Here is the table before filling in any information, with just the
row names.

![Blank sheet with column names](../imgs/excel2.png)

For the specific features within a data set, spaces, capital letters, and
other special characters are fine. Just be consistent. Do not use "female" in
one observation, "Female" in another, and "F" in a third. Just pick the format
and stick to it. Where applicable, try to use standards-based labels (ISO
country codes or U.S. Postal service state abbreviations). This will help if
you later want to merge your data set with other sources.

## One thing in each cell

Within each cell, there should only be one piece of information. In particular,
this means that cells should not contain units or currency symbols. If you have
data collected in different units, create a new column and put the units there.
Though, it is best to store everything on the same scale.
If there is something to note about a particular value, do not put a star or
other mark with it. Create a new column. This also means that,
as mentioned above, you should not try to store two things in one cell by using
formatting to indicate a secondary piece of information. Again, if you have two
things to indicate, create a new column. Here is an example of our cookbook
data set with these principles applied:

![Cookbook data set with values filled](../imgs/excel3.png)

If you need to include explanatory notes for some of the data, which is often
a great idea, do not abandon the rectangular data format. Instead, include an
extra column of notes with its own name. For example, here we explain that one
of our books is out of print:

![Example use for `notes` column](../imgs/excel4.png)

In our example table, the number of pages and weight of one book is missing
because it is out of print. In order to indicate this, the corresponding cell
is blank. Blank values are the only cross-software way to indicate missing
values in a consistent way.

## Data validation tools

Both Excel and Google Sheets have built-in data validation tools that can be
used to ensure that the data you are collecting is consistent. For example, you
can set up a list of valid values for a categorical feature, and then Excel
will only allow you to enter values from that list. This can be a great way to
ensure that you are not introducing errors into your data set.

Here are instructions [for Excel](https://support.microsoft.com/en-us/office/apply-data-validation-to-cells-29fecbcc-d1b9-42c1-9d76-eff3ce5f7249) and [for Google Sheets](https://support.google.com/docs/answer/186103?hl=en&sjid=17134032592351930103-NA).

Let's suppose you are labeling food groups for foods in our familiar `foods`
data set. Each `item` has a unique name, but they are all members of only a few
`food_group`s. If you were to type all of the `food_group`s by hand, there would
likely be typos or differences in the way items are entered (e.g., `"Meat"` vs.
`"meat"`) that would cause trouble during the analysis.

Data validation largely prevents these problems. For some data validation tools, you pick from one of a list of valid values:

![Data validation user interface in Excel](../imgs/validation-1.png){width=50%}

To set this up, I created a separate sheet with values for validation:

![Data validation values](../imgs/validation-2.png){width=50%}

Then, I applied the validation to a column, with a window that looks like this:

![Data validation modal in Excel](../imgs/validation-3.png){width=50%}

Now, when I try to enter a value that is not in the list, Excel will not allow it:

![Data validation error in Excel](../imgs/validation-4.png){width=50%}

:::{.callout-note title="An ounce of prevention `>=` a pound of cure"}
Benjamin Franklin famously argued that, “An ounce of prevention is worth a pound of cure.” By this, he meant that it is ultimately easier to prevent a problem from occurring than it is to fix it later. Data validation is an ounce of prevention that will save you from needing more than a pound of cure!
:::

## Dates and times

Date features are a well-known source of error in the collecting and recording
of data. My recommendation for the most flexible and least error-prone method
is to simply record each component of a date as its own column: `year`, `month`, `day`, `hours`, `minutes`, `seconds`, etc. as needed for your purposes. For `month` in particular, will be *much* easier to work with if you keep months as numbers (e.g., `3`) rather than names (`"March"`). For example, consider trying
to `arrange` by `month`: With month names, they would sort alphabetically,
not chronologically.

One benefit of this method is that it will be easy to record historical data
in cases where you may not be sure of the month or day for every row of the
data set. For example, here is a data set showing properties of the cookbook
authors from our data set:

![Missing data examples](../imgs/excel5.png)

There is an ISO standard (ISO 8601) for representing dates and times in a
consistent way: `YYYY-MM-DD`. This is often a great format for
storing data, and one that is used in several example data sets this semester,
but (1) can lead to errors when opening and re-saving in a spreadsheet program
and (2) cannot easily store dates with unknown information. I suggest using the
separate column approach while collecting your initial data set. Later, if you
re-save a modified data set *from within R*, the ISO 8601 format is a good
option.

## Output format

Most sources on collecting data suggest storing your results in a plain text
format. This is a stripped down representation of the data that contains no
formatting information and is application agnostic. Excel, Google Sheets,
LibreOffice, and any other spreadsheet program you come across, should be able
to save your data set in a plain text format. The most commonly used type is
called a comma separated value (or `.csv`) file. Here, columns are split by
commas and each row is on its own line. Here is what the `.csv` file of our
cookbook authors data dictionary looks like:

```
feature,long_name,units,description
book_title,Book Title,,"Book name, given in English"
author,Author's name,,Authors given and family name
pages,Number of pages,,Pages in currently available edition
weight,Book weight,grams,Weight of currently available edition
nationality,Author's Nationality,,"Using ISO two letter country codes (i.e., CA)"
birth_year,Author's birth year,,As numeric feature
birth_month,Author's birth month,,As numeric feature
birth_day,Author's birth day,,As numeric feature
```

Nearly all of the data sets we work with in this class are stored as CSV files.
One thing to be careful of, particularly when using Excel, is that if your computer
is configured for a language that uses a comma as a decimal separator, the default
csv output may actually use a semicolon (`;`) in place of a comma. To read these
files in R, just replace the `read_csv` with the function `read_csv2`.

### The value of Excel and/or Google Sheets filetypes

Unlike some other sources, I am less strict about the need to only export data
as a plain text file. This is the best way for sharing and storing a data set once
an analysis is finished, but if you are going to continue adding and changing
your data set, it may actually be preferable to store your data in an Excel (`.xlsx`) or Google Sheets file. This can avoid errors that are introduced when converting back and forth between excel and plain text formats.

In R, data can be loaded directly from an Excel file with the `readxl` package. Here is the syntax for using the package to read in a data set, with either the first sheet or a named sheet:

```{r, eval=FALSE}
library(readxl)
# load the first sheet:
data <- read_excel("authors.xlsx")
# load sheet by name:
data <- read_excel("authors.xlsx", sheet = "sheetname") 
```

The easiest way to load data from a Google Sheet is to download the sheet as a
`.csv` file and then read it in with `read_csv`. If you are interested in
repeatedly reading the same Google Sheet, you can use the [`googlesheets4` library](https://github.com/tidyverse/googlesheets4), which is part of the `tidyverse`.

### When to store data in `.csv`

Once you are finished with data collection, cleaning, and processing, then
it is a good idea to store your data in a plain text format for sharing and
long-term preservation. `.csv` is the plain-text format we will use in this class, and probably the most widely used format in data science.

## Data dictionary

So far, our discussion has focused on the specifics of storing the data itself.
It is also important to document exactly what information is being stored in
your features. To do this, we can construct a *data dictionary*. This should
explain, for each feature, basic information such as the feature's name,
measurement units, and expected categorical values. Any decisions that needed to
be made should also be documented. A data dictionary can be a simple text file,
or can be stored itself as a structured data set. Here is an example of a data
dictionary for our authors data set:

![Data dictionary sample](../imgs/excel6.png)

We included a long name for each feature, which will be useful when creating
improved graphics labels when preparing data visualizations for publication.

## Summary

We've covered a lot of information here. For future
reference, here are the key formatting guidelines for storing data sets in
spreadsheets:

- record a single row of column names, starting in cell A1, followed by rows
of observations
- only use lowercase letters, numbers, and underscores in column names; always
start the name with a letter, and keep the feature names relatively short
- when recording categorical features, keep the codes consistent
  - data validation is the **recommended** way of handling this
- only one thing in each cell
- no units, currency symbols, or notes in cells
- keep a separate notes column if needed to explain observations
- blank cells within the rectangle are used if and only if the data is missing
- save dates by storing year, month, day, hour, etc. as their own columns
  - you only need to store time data if it is relevant to your analysis
- save results as either an `.xlsx` file if working in Excel or in a Google Sheet while in the middle of the data collection process
- save as a `.csv` file for long-term storage and sharing
- create a data dictionary to record important information about your data set

As mentioned in the introduction, this is an opinionated list and some other
options are equally valid. The most important thing, however, is consistency,
so I will expect that you follow the formatting advice here when collecting data.

## No homework questions!

Since you had an exam last class, no homework questions! We will practice creating data in class.
